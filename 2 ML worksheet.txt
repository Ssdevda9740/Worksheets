1. Give short description each of Linear, RBF, Polynomial kernels used in SVM.
Linear Kernel is used when the data is Linearly separable, that is, it can be separated using a single Line.
It is one of the most common kernels to be used. It is mostly used when there are a Large number of Features
in a particular Data Set. One of the examples where there are a lot of features, is Text Classification, 
as each alphabet is a new feature. So we mostly use Linear Kernel in Text Classification
 
SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. 
The parameter C, common to all SVM kernels, trades off misclassification of training examples against 
simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at 
classifying all training examples correctly. gamma defines how much influence a single training example has. 
The larger gamma is, the closer other examples must be to be affected.
Proper choice of C and gamma is critical to the SVM’s performance. 
One is advised to use sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far 
apart to choose good values.

In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs)
and other kernelized models, that represents the similarity of vectors (training samples)
in a feature space over polynomials of the original variables, allowing learning of non-linear models.

2. R-squared or Residual Sum of Squares (RSS) which one of these two is a better measure of goodness of fit of model in regression and why?

Residuals identify the deviation of observed values from the expected values.
They are also referred to as error or noise terms. A residual gives an insight into how good our model is 
against the actual value but there are no real-life representations of residual values. 

 “Goodness-of-fit” is a mathematical model that describes the differences between the observed values and 
the expected values or how well the model fits a set of observations. 
This measure can be used in statistical hypothesis testing. 

According to statisticians, if the differences between the observations and the predicted values 
tend to be small and unbiased, we can say that the model fits the data well. 
The meaning of unbiasedness in this context is that the fitted values do not reach the extremes, 
i.e. too high or too low during observations

R-squared is the proportion of variance in the dependent variable that can be explained by the independent variable.
R^2 = (Variance explained by the model/ Total variance)
The value of R-squared stays between 0 and 100%: 
0% corresponds to a model that does not explain the variability of the response data around its mean. 
The mean of the dependent variable helps to predict the dependent variable and also the regression model. 
On the other hand, 100% corresponds to a model that explains the variability of the response variable around its mean. 

Although R-squared is a very intuitive measure to determine how well a regression model fits a dataset, 
it does not narrate the complete story. If you want to get the full picture,
you need to have an in-depth knowledge of R2  along with other statistical measures and residual plots.

3. What are TSS (Total Sum of Squares), ESS (Explained Sum of Squares) and RSS (Residual Sum of Squares) in regression. Also mention the equation relating these three metrics with each other.

The Total SS (TSS or SST) tells you how much variation there is in the dependent variable.
Total SS = Σ(Yi – mean of Y)2.
Note: Sigma (Σ) is a mathematical term for summation or “adding up.” 
It’s telling you to add up all the possible results from the rest of the equation.
Sum of squares is a measure of how a data set varies around a central number (like the mean). 
You might realize by the phrase that you’re summing (adding up) squares—but squares of what? You’ll 
sometimes see this formula:
y= Y-Y'
The Explained SS tells you how much of the variation in the dependent variable your model explained.
Explained SS = Σ(Y-Hat – mean of Y)^2.
The residual sum of squares tells you how much of the dependent variable’s variation your model did not explain. 
It is the sum of the squared differences between the actual Y and the predicted Y:
Residual Sum of Squares = Σ e2

4. What is Gini –impurity index?
Gini index or Gini impurity measures the degree or probability of a particular variable being wrongly classified when 
it is randomly chosen. But what is actually meant by ‘impurity’? If all the elements belong to a single class, 
then it can be called pure. The degree of Gini index varies between 0 and 1, where 0 denotes that all elements belong
 to a certain class or if there exists only one class, and 1 denotes that the elements are randomly distributed across
 various classes. A Gini Index of 0.5 denotes equally distributed elements into some classes.

5. Are unregularized decision-trees prone to overfitting? If yes, why?
Decision trees are prone to overfitting, especially when a tree is particularly deep. 
This is due to the amount of specificity we look at leading to smaller sample of events that meet the previous assumptions.
This small sample could lead to unsound conclusions. An example of this could be predicting if the Boston Celtics
will beat the Miami Heat in tonight’s basketball game. The first level of the tree could ask if the Celtics are 
playing home or away. The second level might ask if the Celtics have a higher win percentage than their opponent, 
in this case the Heat. The third level asks if the Celtic’s leading scorer is playing? The fourth level asks 
if the Celtic’s second leading scorer is playing. The fifth level asks if the Celtics are traveling back to the 
east coast from 3 or more consecutive road games on the west coast. While all of these questions may be relevant, 
there may only be two previous games where the conditions of tonights game were met. 
Using only two games as the basis for our classification would not be adequate for an informed decision. 
One way to combat this issue is by setting a max depth. This will limit our risk of overfitting; but as always,
 this will be at the expense of error due to bias. Thus if we set a max depth of three, we would only ask if the game is
 home or away, do the Celtics have a higher winning percentage than their opponent, and is their leading scorer playing.
This is a simpler model with less variance sample to sample but ultimately will not be a strong predictive model.

6.What is an ensemble technique in machine learning?

Let’s understand the concept of ensemble learning with an example. Suppose you are a movie director and you have created a short movie on a very important and interesting topic. Now, you want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which you can do that?

A: You may ask one of your friends to rate the movie for you.
Now it’s entirely possible that the person you have chosen loves you very much and doesn’t want to break your heart by providing a 1-star rating to the horrible work you have created.

B: Another way could be by asking 5 colleagues of yours to rate the movie.
This should provide a better idea of the movie. This method may provide honest ratings for your movie. But a problem still exists. These 5 people may not be “Subject Matter Experts” on the topic of your movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.

C: How about asking 50 people to rate the movie?
Some of which can be your friends, some of them can be your colleagues and some may even be total strangers.

The responses, in this case, would be more generalized and diversified since now you have people with different sets of skills. And as it turns out – this is a better approach to get honest ratings than the previous cases we saw.

With these examples, you can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called Ensemble Learning.


7. What is an ensemble technique in machine learning?
Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees. As a result, we get an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree classifier.

Bagging Steps:
Suppose there are N observations and M features in training data set. A sample from training data set is taken 
randomly with replacement.
A subset of M features are selected randomly and whichever feature gives the best split is used to 
split the node iteratively.
The tree is grown to the largest.
Above steps are repeated n times and prediction is given based on the aggregation of predictions from n number of trees.

Advantages:
Reduces over-fitting of the model.
Handles higher dimensionality data very well.
Maintains accuracy for missing data.

Disadvantages:
Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for 
the classification and regression model.

Python Syntax:
rfm = RandomForestClassifier(n_estimators=80, oob_score=True, n_jobs=-1, random_state=101, 
max_features = 0.50, min_samples_leaf = 5)
fit(x_train, y_train)
predicted = rfm.predict_proba(x_test)

Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially 
with early learners fitting simple models to the data and then analysing data for errors. 
Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. 
When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model.

Boosting Steps:
Draw a random subset of training samples d1 without replacement from the training set D to train a weak learner C1
Draw second random training subset d2 without replacement from the training set 
and add 50 percent of the samples that were previously falsely classified/misclassified to train a weak learner C2
Find the training samples d3 in the training set D on which C1 and C2 disagree to train a third weak learner C3
Combine all the weak learners via majority voting.

Advantages:
Supports different loss function (we have used ‘binary:logistic’ for this example).
Works well with interactions.

Disadvantages:
Prone to over-fitting.
Requires careful tuning of different hyper-parameters.

Python Syntax:
from xgboost import XGBClassifier
xgb = XGBClassifier(objective=’binary:logistic’, n_estimators=70, seed=101)
fit(x_train, y_train)
predicted = xgb.predict_proba(x_test)

8. what is out-of-bag error in random forests

Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests,
boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging) to sub-sample data 
samples used for training.

9. What is K-fold cross-validation?
Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.

The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.

Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.

It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.

The general procedure is as follows:

Shuffle the dataset randomly.
Split the dataset into k groups
For each unique group:
Take the group as a hold out or test data set
Take the remaining groups as a training data set
Fit a model on the training set and evaluate it on the test set
Retain the evaluation score and discard the model
Summarize the skill of the model using the sample of model evaluation scores
Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.

10. What is hyper parameter tuning in machine learning and why it is done?

A hyperparameter is a parameter whose value is set before the learning process begins.
Some examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.
In sklearn, hyperparameters are passed in as arguments to the constructor of the model classes.
Tuning Strategies
We will explore two different methods for optimizing hyperparameters:
Grid Search
Random Search
We’ll begin by preparing the data and trying several different models with their default hyperparameters. From these we’ll select the top two performing methods for hyperparameter tuning

Grid Search
Grid search is a traditional way to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.
Using sklearn’sGridSearchCV, we first define our grid of parameters to search over and then run the grid search.

Random Search
Random search differs from grid search mainly in that it searches the specified subset of hyperparameters randomly instead of exhaustively. The major benefit being decreased processing time.
There is a tradeoff to decreased processing time, however. We aren’t guaranteed to find the optimal combination of hyperparameters.
Let’s give random search a try with sklearn’s RandomizedSearchCV. Very similar to grid search above, we define the hyperparameters to search over before running the search.

11. What issues can occur if we have a large learning rate in Gradient Descent?
When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error. […] When the learning rate is too small, training is not only slower, but may become permanently stuck with a high training error.

12. What is bias-variance trade off in machine learning?
Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance

13. What is the need of regularization in machine learning?

Regularisation is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting.
The commonly used regularisation techniques are :

L1 regularisation
L2 regularisation
Dropout regularisation

14. Differentiate between Adaboost and Gradient Boosting?

Adaboost is more about ‘voting weights’ and Gradient boosting is more about ‘adding gradient optimization’. 

Adaboost increases the accuracy by giving more weightage to the target which is misclassified by the model. At each iteration, Adaptive boosting algorithm changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances.

Gradient boosting calculates the gradient (derivative) of the Loss Function with respect to the prediction (instead of the features). Gradient boosting increases the accuracy by minimizing the Loss Function (error which is difference of actual and predicted value) and having this loss as target for the next iteration.

Gradient boosting algorithm builds first weak learner and calculates the Loss Function. It then builds a second learner to predict the loss after the first step. The step continues for third learner and then for fourth learner and so on until a certain threshold is reached.

15. Can we use Logistic Regression for classification of Non-Linear Data? If not, why?

Logistic regression has traditionally been used to come up with a hyperplane that separates the feature space into classes. But if we suspect that the decision boundary is nonlinear we may get better results by attempting some nonlinear functional forms for the logit function